{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allen Wong Machine Learning Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on the Airbnb New User Bookings Kaggle Challenge (site: https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings). The notebook will be divided into three parts: training set, testing set and generating output for kaggle submission. I will be performing: \n",
    "\n",
    "- Training Set\n",
    " - Data Pre-processing; \n",
    " - Training on several tree based models; \n",
    " - Tuning the hyperparameters through Grid Search and Random Search;\n",
    " - Training the optimal model\n",
    " - Identifying the 5 most important features\n",
    " - Comparing the performance of the full model and reduced model\n",
    "\n",
    "\n",
    "- Testing Set\n",
    " - Data Preprocessing;\n",
    " - Evaluating the full model against the test set\n",
    " \n",
    " \n",
    "- Submission to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "\n",
    "#import starting pre-processed files\n",
    "aggregated_web_sessions = pd.read_csv(\"Aggregation of web session log details 12.5.17 101pm ET (with user and time secs elapsed mapping).csv\") #processed web sessions data, further details in capstone writeup\n",
    "train_users_AWprocessed = pd.read_csv('train_users_2_AW_processed.csv') #processed train users file, e.g. deleted any users age greater than 115. further details in capstone writeup\n",
    "train_users_joined = pd.merge(train_users_AWprocessed, aggregated_web_sessions, left_on = 'user_id', right_on = 'user_id', how = 'left') #combining web sessions data with train users data\n",
    "\n",
    "#additional cleanups of pre-processed files\n",
    "del train_users_joined['timestamp_first_active'] #assumed that this field is not important as there are only one values for all users\n",
    "train_users_joined['age'].fillna(-1, inplace=True) #any users with missing age are replaced with a -1 value. The negative sign is there to help the model know that these values are completely different from all other values\n",
    "train_users_joined['first_affiliate_tracked'].fillna('missing', inplace=True) \n",
    "train_users_joined.fillna(-1, inplace=True) \n",
    "\n",
    "#write to excel and reorder so that the 15,500 users are placed on the top first. Reordered the users based on whether secs elapsed is a positive value or -1 (missing from the data)\n",
    "#writer = pd.ExcelWriter('train_users_joined(placed 15500 first)_test.xlsx') #already wrote these files when running this. showing code for illustrative purposes\n",
    "#train_users_joined.to_excel(writer,'Sheet1')\n",
    "#writer.save()\n",
    "\n",
    "#import the csv files with 15,500 users reordered\n",
    "train_users_final = pd.read_csv('train_users_joined(placed 15500 first)_test3 12.24.17 430pm ET.csv')\n",
    "\n",
    "#bring in the coordinates and distance km2 information that the 213,451 training users will all see\n",
    "train_users_final_coordinates = pd.read_csv('Countries 213451.csv')\n",
    "\n",
    "#join the 15,500 reordered users file with the Countries data\n",
    "train_users_final_join_coordinates = pd.concat([train_users_final, train_users_final_coordinates], axis=1, join='inner')\n",
    "\n",
    "\n",
    "#split the resulting data first so that the first 15500 users are separated out. The remaining data will then be shuffled and these two \n",
    "#dataframes will be joined back together \n",
    "train_users_final_1of2 = train_users_final_join_coordinates[0:15500] #first 15500 users\n",
    "train_users_final_2of2 = train_users_final_join_coordinates[15500:] #split these users out and shuffle\n",
    "\n",
    "train_users_final_2of2 = train_users_final_2of2.sample(frac=1) #shuffle this dataframe in case it needs to be shuffled. The shuffle function will be turned off later in splitting the training and test data\n",
    "\n",
    "#rejoined the two split dataframes back together\n",
    "train_users_final_after_shuffle = pd.concat([train_users_final_1of2, train_users_final_2of2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features & Target Label Splitting, One-Hot Encoding and Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is almost ready at this point. All of the necessary information has been combined into one file and the users have been reordered such that only the users with web sessions activity appear first. This will be necessary when we split the data between the training set and the cross validation set; we want the data distribution of the cross validation set to be similar to the data distribution of the final test set. The following steps will get our data ready for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "#delete user_id first so that one-hot encoding does not throw a memory error\n",
    "del train_users_final_after_shuffle['user_id']\n",
    "\n",
    "# Split the data into features and target label\n",
    "country_raw = train_users_final_after_shuffle['country_destination']\n",
    "features_raw = train_users_final_after_shuffle.drop('country_destination', axis = 1)\n",
    "\n",
    "# One-hot encode features and country data\n",
    "features_one_hot = pd.get_dummies(features_raw)\n",
    "\n",
    "# Encode the 'country_raw' data to numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "le.fit(country_raw)\n",
    "country = le.transform(country_raw)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features_one_hot.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are currently stored as a numpy. We need to take the following additional steps to convert it from a numpy to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the column labels. \n",
    "columns_of_features_one_hot = features_one_hot.dtypes.index #Ref: https://stackoverflow.com/questions/24901766/python-how-to-get-column-names-from-pandas-dataframe-but-only-for-continuous\n",
    "\n",
    "index_of_features_one_hot=range(0,213451) #Get the row labels.\n",
    "\n",
    "#Ref: https://stackoverflow.com/questions/35723472/how-to-use-sklearn-fit-transform-with-pandas-and-return-dataframe-instead-of-num\n",
    "#Ref: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n",
    "#Ref: http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "features_scaled = RobustScaler().fit_transform(features_one_hot.values)\n",
    "\n",
    "features_scaled_df = pd.DataFrame(features_scaled, index=index_of_features_one_hot, columns=columns_of_features_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! We are ready to split the dataset between the training and cross validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset between Training and Cross Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 170760 samples.\n",
      "Testing set has 42691 samples.\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Now that the 15,500 users with web sessions are on the top and the remaining users have been shuffled, we can call train_test_split with shuffle equals false so\n",
    "#we make sure that those 15,500 users are in the training set and the validation set has the same data distribution as the test set\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled_df, \n",
    "                                                    country, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle = False)\n",
    "\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on Several Tree Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is ready for training now!I will be applying 6 different models to the training set. They are:\n",
    " - AdaBoost \n",
    " - Bagging \n",
    " - Decision Tree\n",
    " - ExtraTrees\n",
    " - Gradient Boosting\n",
    " - Random Forest\n",
    " \n",
    "I will also be setting a timer for each of the above classifiers to get a sense of each model's efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdaBoostClassifier': [0.9141842247093165, 447.74178647994995], 'BaggingClassifier': [0.904545723784987, 5557.759644508362], 'DecisionTreeClassifier': [0.8693511814527409, 112.72081208229065], 'ExtraTreesClassifier': [0.8981451797755682, 549.6879096031189], 'GradientBoostingClassifier': [0.913931656817523, 27350.038553714752], 'RandomForestClassifier': [0.9047377244170851, 260.956650018692]}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#first define calculation for nDCG\n",
    "def calculate_nDCG(x):\n",
    "    nDCG = [] \n",
    "    for element in x:\n",
    "        nDCG.append((2 - 1)/math.log(element+1, 2))\n",
    "    return nDCG\n",
    "\n",
    "# #This line will go through each element in the maximal_value_tracking_index (which is a list that keeps track of the rank of the predictions)\n",
    "# #and compares it y_test. If a given array has the same value, then a 1 is placed into index_for_DCG, otherwise the rank of a given country in predictions (referencing the correct country from y_test)\n",
    "# #will be appended to index_for_DCG. Ideal case is an index of all 1's\n",
    "#Ref: https://stackoverflow.com/questions/6422700/how-to-get-indices-of-a-sorted-array-in-python\n",
    "#Ref: https://stackoverflow.com/questions/6193498/pythonic-way-to-find-maximum-value-and-its-index-in-a-list\n",
    "#Ref: https://stackoverflow.com/questions/5284646/rank-items-in-an-array-using-python-numpy\n",
    "#Ref: http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import operator\n",
    "import time \n",
    "\n",
    "#convert y_train to the appropriate dimension\n",
    "y2 = y_train\n",
    "Y = np.vstack((y_train, y2)).T\n",
    "\n",
    "clf_A = AdaBoostClassifier(n_estimators= 100, random_state=1)\n",
    "clf_B = BaggingClassifier(n_estimators= 100, random_state=1)\n",
    "clf_C = DecisionTreeClassifier(random_state=1) \n",
    "clf_D = ExtraTreesClassifier(n_estimators= 100, random_state=1)\n",
    "clf_E = GradientBoostingClassifier(n_estimators= 100, random_state=1)\n",
    "clf_F = RandomForestClassifier(n_estimators= 100, random_state=1)\n",
    "\n",
    "results = {} #create a dictionary for the results\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F]:\n",
    "    tic = time.time()      #start the timer\n",
    "    clf_name = clf.__class__.__name__ #assign the classifier name to the clf_name variable\n",
    "    clf_specific = clf \n",
    "    \n",
    "    multi_target = MultiOutputClassifier(clf_specific, n_jobs=-1) #jobs is the number of core to use. Since it is set to -1, all cores are being used\n",
    "    multi_target.fit(X_train, Y).predict(X_train) \n",
    "    predictions = multi_target.predict_proba(X_test)[0] #this will return an array with 12 columns that predicts the probability of the new user going to each country\n",
    "    maximal_value_tracking_index =[]\n",
    "\n",
    "    for element in predictions:\n",
    "        order = element.argsort()\n",
    "        rank = order.argsort()\n",
    "        maximal_value_tracking_index.append(rank+1)\n",
    "\n",
    "    \n",
    "    index_for_DCG =[]\n",
    "    \n",
    "    i=0\n",
    "    for rank_element in maximal_value_tracking_index:\n",
    "        if rank_element[y_test[i]] == 12: #maximal probability\n",
    "            index_for_DCG.append(1)\n",
    "        else:\n",
    "            if rank_element[y_test[i]] == 1:  #1 is a special number to mean that both the prediction and true value matches. In this line, 1 means the prediction was ranked last\n",
    "                index_for_DCG.append(12)\n",
    "            else:\n",
    "                index_for_DCG.append(rank_element[y_test[i]])        \n",
    "        i=i+1\n",
    "\n",
    "    calculate_nDCG(index_for_DCG)\n",
    "    toc = time.time() #stop the timer \n",
    "    total_time = toc - tic \n",
    "    results[clf_name] = [sum(calculate_nDCG(index_for_DCG))/float(len(calculate_nDCG(index_for_DCG))), total_time]\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which is the best classifier, I will employ Grid Search and Random Search to find the best hyperparameters. For both search, I will be using accuracy as the scorer, since this function is already built-in and it would also give me a better sense of how the model is performing rather than the somewhat inflated nDCG score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameters - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.8752\n",
      "{'learning_rate': 0.8, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = AdaBoostClassifier(random_state=1)\n",
    "\n",
    "# Parameters to tune\n",
    "parameters = {'n_estimators': [50, 200, 300, 400, 500], \n",
    "              'learning_rate': [0.2, 0.4, 0.6, 0.8, 1], \n",
    "              }\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report accuracy score and print the grid object with the best parameters\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's been said that Random Search is a better search than Grid Search. Let's see if the results differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameters - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.8752\n",
      "{'n_estimators': 50, 'learning_rate': 0.8}\n"
     ]
    }
   ],
   "source": [
    "#Import 'RandomizedSearchCV'\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize the classifier\n",
    "clf_Rand = AdaBoostClassifier(random_state=1)\n",
    "\n",
    "# Parameters to tune\n",
    "parameters = {'n_estimators': [50, 200, 300, 400, 500], \n",
    "              'learning_rate': [0.2, 0.4, 0.6, 0.8, 1], \n",
    "              }\n",
    "\n",
    "# Perform randomized search on the classifier using 'scorer' as the scoring method using RandomizedSearchCV()\n",
    "grid_obj_Rand = RandomizedSearchCV(clf_Rand, parameters, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit_Rand = grid_obj_Rand.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf_Rand = grid_fit_Rand.best_estimator_\n",
    "\n",
    "# Make predictions using the optimized model\n",
    "best_predictions_Rand = best_clf_Rand.predict(X_test)\n",
    "\n",
    "# Report accuracy score and print the grid object with the best parameters\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions_Rand)))\n",
    "print(grid_obj_Rand.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the appropriate hyperparameters, let's run the optimal model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdaBoostClassifier': [0.9141609121322275, 155.86502981185913]}\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/34214087/how-do-you-access-tree-depth-in-pythons-scikit-learn#Running Optimal Model\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import operator\n",
    "import time \n",
    "\n",
    "#convert y_train to the appropriate dimension\n",
    "y2 = y_train\n",
    "Y = np.vstack((y_train, y2)).T\n",
    "\n",
    "clf_Optimized = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1)\n",
    "\n",
    "results = {} #create a dictionary for the results\n",
    "for clf in [clf_Optimized]:\n",
    "    tic = time.time()      #start the timer\n",
    "    clf_name = clf.__class__.__name__ #assign the classifier name to the clf_name variable\n",
    "    clf_specific = clf \n",
    "    \n",
    "    multi_target = MultiOutputClassifier(clf_specific, n_jobs=-1) #jobs is the number of core to use. Since it is set to -1, all cores are being used\n",
    "    multi_target.fit(X_train, Y).predict(X_train) \n",
    "    predictions = multi_target.predict_proba(X_test)[0] #this will return an array with 12 columns that predicts the probability of the new user going to each country\n",
    "    maximal_value_tracking_index =[]\n",
    "\n",
    "    for element in predictions:\n",
    "        order = element.argsort()\n",
    "        rank = order.argsort()\n",
    "        maximal_value_tracking_index.append(rank+1)\n",
    "\n",
    "    \n",
    "    index_for_DCG =[]\n",
    "    \n",
    "    i=0\n",
    "    for rank_element in maximal_value_tracking_index:\n",
    "        if rank_element[y_test[i]] == 12: #maximal probability\n",
    "            index_for_DCG.append(1)\n",
    "        else:\n",
    "            if rank_element[y_test[i]] == 1:  #1 is a special number to mean that both the prediction and true value matches. In this line, 1 means the prediction was ranked last\n",
    "                index_for_DCG.append(12)\n",
    "            else:\n",
    "                index_for_DCG.append(rank_element[y_test[i]])        \n",
    "        i=i+1\n",
    "\n",
    "    calculate_nDCG(index_for_DCG)\n",
    "    toc = time.time() #stop the timer \n",
    "    total_time = toc - tic \n",
    "    results[clf_name] = [sum(calculate_nDCG(index_for_DCG))/float(len(calculate_nDCG(index_for_DCG))), total_time]\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to see what are the top 5 most critical features of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the 5 Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoMAAAFgCAYAAADAVnM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fab384d198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run the optimal model\n",
    "model = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "# Extract the feature importances using .feature_importances_ \n",
    "importances = model.feature_importances_\n",
    "\n",
    "import visuals2 as vs #this file was from the finding donors project\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a better reading of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date_first_booking_monthyear_missingmissingY' 'secs_elapsed'\n",
      " 'Date_account_created_MonthYear_14-Mar'\n",
      " 'date_first_booking_monthyear_December2013Y'\n",
      " 'Date_account_created_MonthYear.1_May2014Y']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "columns = X_train.columns.values[indices[:]]\n",
    "print(columns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the performance of the full model against the reduced model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Full Model vs. Reduced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model trained on full data\n",
      "------\n",
      "Accuracy on testing data: 0.8752\n",
      "\n",
      "Final Model trained on reduced data\n",
      "------\n",
      "Accuracy on testing data: 0.8752\n"
     ]
    }
   ],
   "source": [
    "# Reduce the feature space\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]] #from finding donors\n",
    "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]] #from finding donors\n",
    "\n",
    "# Train on the \"best\" model found from random search earlier\n",
    "clf_reduced = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1).fit(X_train_reduced, y_train)\n",
    "clf_full = AdaBoostClassifier(n_estimators=50, learning_rate=0.8, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "# Make new predictions\n",
    "reduced_predictions = clf_reduced.predict(X_test_reduced)\n",
    "full_predictions = clf_full.predict(X_test)\n",
    "\n",
    "print(\"Final Model trained on full data\\n------\")\n",
    "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, full_predictions)))\n",
    "print(\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now turn to the test set and examine how our model will perform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import somewhat pre-processed test set\n",
    "test_users_AWprocessed = pd.read_csv('test_usersAW processed 12.31.17 1059am ET.csv')\n",
    "\n",
    "#similar to before, bring in the coordinates and distance km2 of destination countries\n",
    "test_users_final_coordinates = pd.read_csv('Countries 62096.csv')\n",
    "\n",
    "#join the two datasets\n",
    "test_users_final_join_coordinates = pd.concat([test_users_AWprocessed, test_users_final_coordinates], axis=1, join='inner')\n",
    "\n",
    "#both the test set and training set need to have the same number of features for the model to run. 103 features needed to be added to the test set. These dummy features have a value of 0\n",
    "test_dummy_features = pd.read_csv('103 dummy features.csv')\n",
    "\n",
    "#join all the data together\n",
    "test_users_final_join_coordinates_f = pd.concat([test_users_final_join_coordinates, test_dummy_features], axis=1, join='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features & Target Label Splitting, One-Hot Encoding and Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "#delete the user_id to prevent it from throwing a memory error\n",
    "del test_users_final_join_coordinates_f['id']\n",
    "\n",
    "# Split the data into features and target label\n",
    "country_raw_test = test_users_final_join_coordinates_f['country_destination']\n",
    "features_raw_test = test_users_final_join_coordinates_f.drop('country_destination', axis = 1)\n",
    "\n",
    "# One-hot encode features and country data\n",
    "features_one_hot_test = pd.get_dummies(features_raw_test)\n",
    "\n",
    "# Encode the 'country_raw' data to numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_test= LabelEncoder()\n",
    "le_test.fit(country_raw_test)\n",
    "country_test = le.transform(country_raw_test)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded_test = list(features_one_hot_test.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the column labels. This is for the step where we convert the scaled numpy to a dataframe\n",
    "columns_of_features_one_hot_test = features_one_hot_test.dtypes.index #Ref: https://stackoverflow.com/questions/24901766/python-how-to-get-column-names-from-pandas-dataframe-but-only-for-continuous\n",
    "index_of_features_one_hot_test=range(0,62096) #Get the row labels. This is for the step where we convert the scaled numpy to a dataframe\n",
    "\n",
    "#Ref: https://stackoverflow.com/questions/35723472/how-to-use-sklearn-fit-transform-with-pandas-and-return-dataframe-instead-of-num\n",
    "\n",
    "features_scaled_test = RobustScaler().fit_transform(features_one_hot_test.values)\n",
    "\n",
    "features_scaled_df_test = pd.DataFrame(features_scaled_test, index=index_of_features_one_hot_test, columns=columns_of_features_one_hot_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 62096 samples.\n",
      "Testing set has 0 samples.\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Now that the 15,500 users with web sessions are on the top and the remaining users have been shuffled, we can call train_test_split with shuffle equals false so\n",
    "#we make sure that those 15,500 users are in the training set and the validation set has the same data distribution as the test set\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(features_scaled_df_test, \n",
    "                                                    country_test, \n",
    "                                                    test_size = 0, \n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle = False)\n",
    "\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train_t.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test_t.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdaBoostClassifier': [1.0, 169.42703485488892]}\n"
     ]
    }
   ],
   "source": [
    "#convert y_train to the appropriate dimension\n",
    "y2 = y_train\n",
    "Y = np.vstack((y_train, y2)).T\n",
    "\n",
    "clf_A_t = AdaBoostClassifier(n_estimators= 50, learning_rate=0.8, random_state=1)\n",
    "\n",
    "results = {} #create a dictionary for the results\n",
    "for clf in [clf_A_t]:\n",
    "    tic = time.time()      #start the timer\n",
    "    clf_name = clf.__class__.__name__ #assign the classifier name to the clf_name variable\n",
    "    clf_specific = clf \n",
    "    \n",
    "    multi_target = MultiOutputClassifier(clf_specific, n_jobs=-1) #jobs is the number of core to use. Since it is set to -1, all cores are being used\n",
    "    multi_target.fit(X_train, Y).predict(X_train) \n",
    "    predictions = multi_target.predict_proba(X_train_t)[0] #this will return an array with 12 columns that predicts the probability of the new user going to each country\n",
    "    maximal_value_tracking_index =[]\n",
    "\n",
    "    for element in predictions:\n",
    "        order = element.argsort()\n",
    "        rank = order.argsort()\n",
    "        maximal_value_tracking_index.append(rank+1)\n",
    "\n",
    "    \n",
    "    index_for_DCG =[]\n",
    "    \n",
    "    i=0\n",
    "    for rank_element in maximal_value_tracking_index:\n",
    "        if rank_element[y_train_t[i]] == 12: #maximal probability\n",
    "            index_for_DCG.append(1)\n",
    "        else:\n",
    "            if rank_element[y_train_t[i]] == 1:  #1 is a special number to mean that both the prediction and true value matches. In this line, 1 means the prediction was ranked last\n",
    "                index_for_DCG.append(12)\n",
    "            else:\n",
    "                index_for_DCG.append(rank_element[y_train_t[i]])        \n",
    "        i=i+1\n",
    "\n",
    "    calculate_nDCG(index_for_DCG)\n",
    "    toc = time.time() #stop the timer \n",
    "    total_time = toc - tic \n",
    "    results[clf_name] = [sum(calculate_nDCG(index_for_DCG))/float(len(calculate_nDCG(index_for_DCG))), total_time]\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Output for Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the country labels\n",
    "label_spelled_out = ['AU', 'DE', 'NL', 'PT', 'IT', 'ES', 'GB', 'CA', 'FR', 'other', 'US', 'NDF']\n",
    "\n",
    "#sort the labels\n",
    "sort_label_spelled_out = sorted(label_spelled_out)\n",
    "\n",
    "#create an array with the same shape as the testing set, \n",
    "#Ref: https://stackoverflow.com/questions/1550130/cloning-row-or-column-vectors\n",
    "country_labels = np.tile(np.asarray(sort_label_spelled_out), (62096,1)) \n",
    "\n",
    "#remove \"array\" from beginning and \"dtype\" at end\n",
    "maximal_value_tracking_index = np.asarray(maximal_value_tracking_index) \n",
    "\n",
    "#for each element, in each array, assign country_labels with the value in maximal value tracking. Remember, 12 is the largest and considerd the country most likely to vist\n",
    "#Ref: https://stackoverflow.com/questions/6618515/sorting-list-based-on-values-from-another-list\n",
    "i=0\n",
    "country_labels_maximal_tracking = []\n",
    "for element in country_labels:\n",
    "    country_labels_maximal_tracking.append(list(zip(maximal_value_tracking_index[i],country_labels[i])))\n",
    "    i=i+1\n",
    "    \n",
    "#sort the labels in reverse order\n",
    "for element in country_labels_maximal_tracking:\n",
    "    element.sort(reverse=True)\n",
    "    \n",
    "#extract the country from each subarray in country_labels_maximal_tracking\n",
    "country_labels_sorted =[]\n",
    "for element in list(country_labels_maximal_tracking):\n",
    "    tracking_array = []\n",
    "    i=0\n",
    "    for sub_element in element:  \n",
    "        if i<5:                                     #keep the first 5 elements in each subarray   \n",
    "            tracking_array.append(sub_element[1])\n",
    "            i=i+1\n",
    "        else:\n",
    "            None\n",
    "    country_labels_sorted.append(tracking_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step below will extract the output above and put it into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('AW_top_five_countries_robust_scaler.xlsx')\n",
    "top_five_countries = pd.DataFrame(data=country_labels_sorted) #convert index to dataframe\n",
    "top_five_countries.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RandomForestClassifier': [0.8994321229560059, 50.84402871131897]}\n"
     ]
    }
   ],
   "source": [
    "#convert y_train to the appropriate dimension\n",
    "y2 = y_train\n",
    "Y = np.vstack((y_train, y2)).T\n",
    "\n",
    "clf_benchmark = RandomForestClassifier(random_state=1)\n",
    "\n",
    "results = {} #create a dictionary for the results\n",
    "for clf in [clf_benchmark]:\n",
    "    tic = time.time()      #start the timer\n",
    "    clf_name = clf.__class__.__name__ #assign the classifier name to the clf_name variable\n",
    "    clf_specific = clf \n",
    "    \n",
    "    multi_target = MultiOutputClassifier(clf_specific, n_jobs=-1) #jobs is the number of core to use. Since it is set to -1, all cores are being used\n",
    "    multi_target.fit(X_train, Y).predict(X_train) \n",
    "    predictions = multi_target.predict_proba(X_test)[0] #this will return an array with 12 columns that predicts the probability of the new user going to each country\n",
    "    maximal_value_tracking_index =[]\n",
    "\n",
    "    for element in predictions:\n",
    "        order = element.argsort()\n",
    "        rank = order.argsort()\n",
    "        maximal_value_tracking_index.append(rank+1)\n",
    "\n",
    "    \n",
    "    index_for_DCG =[]\n",
    "    \n",
    "    i=0\n",
    "    for rank_element in maximal_value_tracking_index:\n",
    "        if rank_element[y_test[i]] == 12: #maximal probability\n",
    "            index_for_DCG.append(1)\n",
    "        else:\n",
    "            if rank_element[y_test[i]] == 1:  #1 is a special number to mean that both the prediction and true value matches. In this line, 1 means the prediction was ranked last\n",
    "                index_for_DCG.append(12)\n",
    "            else:\n",
    "                index_for_DCG.append(rank_element[y_test[i]])        \n",
    "        i=i+1\n",
    "\n",
    "    calculate_nDCG(index_for_DCG)\n",
    "    toc = time.time() #stop the timer \n",
    "    total_time = toc - tic \n",
    "    results[clf_name] = [sum(calculate_nDCG(index_for_DCG))/float(len(calculate_nDCG(index_for_DCG))), total_time]\n",
    "    \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
